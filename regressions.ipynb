{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "from cleaning import clean_data\n",
    "from evaluate import qwk\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training = (10495, 335)\n",
      "Testing = (4498, 335)\n"
     ]
    }
   ],
   "source": [
    "# Reduce collinearity of features\n",
    "# Load Data\n",
    "X_train, y_train, X_test, y_test = clean_data('')\n",
    "\n",
    "# PCA Decomposition\n",
    "pca = PCA(svd_solver='full')\n",
    "pcaX_train = pca.fit_transform(X_train)\n",
    "pcaX_test = pca.transform(X_test)\n",
    "\n",
    "# Select K Best\n",
    "kb = SelectKBest()\n",
    "kbX_train = kb.fit_transform(X_train, y_train)\n",
    "kbX_test = kb.transform(X_test)\n",
    "\n",
    "# Variance Threshold\n",
    "vt = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "vtX_train = vt.fit_transform(X_train, y_train)\n",
    "vtX_test = vt.transform(X_test)\n",
    "\n",
    "print(f'Training = {X_train.shape}')\n",
    "print(f'Testing = {X_test.shape}')\n",
    "# Create dict to store outcomes\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Ord. Least Squares ======================\n",
      "train acc: 0.07043568746413342\n",
      "test acc: 0.06251794577351222\n",
      "============= Lasso ======================\n",
      "train acc: -5.849490605447372e-06\n",
      "============= Ridge ======================\n",
      "test acc: 0.06251845952326063\n",
      "============= Elastic ======================\n",
      "train acc: 0.0\n",
      "test acc: -5.849490605447372e-06\n",
      "============= LogReg ======================\n",
      "train acc: 0.1700048895582429\n",
      "test acc: 0.140889442175558\n"
     ]
    }
   ],
   "source": [
    "# Fit linear models with increasing l2 penalties\n",
    "ols = linear_model.LinearRegression()\n",
    "ridge = linear_model.Ridge()\n",
    "lasso = linear_model.Lasso()\n",
    "elastic = linear_model.ElasticNet()\n",
    "\n",
    "\n",
    "\n",
    "logReg = ensemble.GradientBoostingRegressor()\n",
    "logReg = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "ols.fit(kbX_train, y_train)\n",
    "ridge.fit(kbX_train, y_train)\n",
    "lasso.fit(kbX_train, y_train)\n",
    "elastic.fit(kbX_train, y_train)\n",
    "logReg.fit(kbX_train, y_train)\n",
    "ols_pred = ols.predict(kbX_test)\n",
    "ridge_pred = ridge.predict(kbX_test)\n",
    "lasso_pred = lasso.predict(kbX_test)\n",
    "elastic_pred = elastic.predict(kbX_test)\n",
    "\n",
    "def roundGuess(guesses):\n",
    "    for i, guess in enumerate(guesses):\n",
    "        if guess < 0.5:\n",
    "            guesses[i] = 0\n",
    "        elif guess < 1.5:\n",
    "            guesses[i] = 1\n",
    "        elif guess < 2.5:\n",
    "            guesses[i] = 2    \n",
    "        elif guess < 3.5:\n",
    "            guesses[i] = 3\n",
    "        elif guess < 4.5:\n",
    "            guesses[i] = 4\n",
    "        else: guesses[i] = 5\n",
    "    return guesses     \n",
    "\n",
    "# roundedGuess = roundGuess(ols_pred)\n",
    "# print(mean_squared_error(roundedGuess, y_test))\n",
    "\n",
    "print(\"============= Ord. Least Squares ======================\")\n",
    "print(\"train acc: \" + str(ols.score(kbX_train, y_train)))\n",
    "print(\"test acc: \" + str(ols.score(kbX_test, y_test)))\n",
    "# print(\"qwk: \" + str(qwk(y_test, ols_pred)))\n",
    "\n",
    "print(\"============= Lasso ======================\")\n",
    "print(\"train acc: \" + str(lasso.score(kbX_test, y_test)))\n",
    "# print(\"qwk: \" + str(qwk(y_test, lasso_pred)))\n",
    "\n",
    "print(\"============= Ridge ======================\")\n",
    "print(\"test acc: \" + str(ridge.score(kbX_test, y_test)))\n",
    "# print(\"qwk: \" + str(qwk(y_test, ridge_pred)))\n",
    "\n",
    "print(\"============= Elastic ======================\")\n",
    "print(\"train acc: \" + str(elastic.score(kbX_train, y_train)))\n",
    "print(\"test acc: \" + str(elastic.score(kbX_test, y_test)))# print(\"qwk: \" + str(qwk(y_test, elastic_pred)))\n",
    "\n",
    "print(\"============= LogReg ======================\")\n",
    "print(\"train acc: \" + str(logReg.score(kbX_train, y_train)))\n",
    "print(\"test acc: \" + str(logReg.score(kbX_test, y_test)))# print(\"qwk: \" + str(qwk(y_test, elastic_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Logistic Regression ======================\n",
      "test acc: 0.3609337779895188\n",
      "train acc: 0.3554913294797688\n",
      "============= Cross Validated Logistic Regression ======================\n",
      "test acc: 0.3613149118627918\n",
      "train acc: 0.35437972432192083\n",
      "============= SGD Classifier ======================\n",
      "test acc: 0.29909480705097663\n",
      "train acc: 0.29702089817696753\n",
      "============= Gradient Boosted Regressor Classifier ======================\n",
      "test acc: 0.4479275845640781\n",
      "train acc: 0.3930635838150289\n"
     ]
    }
   ],
   "source": [
    "# Fit classification models\n",
    "log = linear_model.LogisticRegression( max_iter=1000)\n",
    "CVlog = linear_model.LogisticRegressionCV(max_iter=1000)\n",
    "SGD = linear_model.SGDClassifier()\n",
    "GBClass = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "log.fit(vtX_train, y_train)\n",
    "CVlog.fit(vtX_train, y_train)\n",
    "SGD.fit(vtX_train, y_train)\n",
    "GBClass.fit(vtX_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"============= Logistic Regression ======================\")\n",
    "print(\"test acc: \" + str(log.score(vtX_train, y_train)))\n",
    "print(\"train acc: \" + str(log.score(vtX_test, y_test)))\n",
    "\n",
    "print(\"============= Cross Validated Logistic Regression ======================\")\n",
    "print(\"test acc: \" + str(CVlog.score(vtX_train, y_train)))\n",
    "print(\"train acc: \" + str(CVlog.score(vtX_test, y_test)))\n",
    "\n",
    "\n",
    "print(\"============= SGD Classifier ======================\")\n",
    "print(\"test acc: \" + str(SGD.score(vtX_train, y_train)))\n",
    "print(\"train acc: \" + str(SGD.score(vtX_test, y_test)))\n",
    "\n",
    "print(\"============= Gradient Boosted Regressor Classifier ======================\")\n",
    "print(\"test acc: \" + str(GBClass.score(vtX_train, y_train)))\n",
    "print(\"train acc: \" + str(GBClass.score(vtX_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Borrow an optimized Rounding function for regression buckets from https://www.kaggle.com/code/abhishek/maybe-something-interesting-here/notebook\n",
    "from functools import partial\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        print(loss_partial)\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA50A56430>>, X=array([2.14977434, 2.67040106, 3.0850211 , ..., 2.53606072, 2.41704703,\n",
      "       2.60152644]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA524359A0>>, X=array([2.14960777, 2.63195443, 3.15550627, ..., 2.63080459, 2.49138859,\n",
      "       2.58038141]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA50A56430>>, X=array([2.18238178, 2.73312203, 3.13962531, ..., 2.53461321, 2.31685351,\n",
      "       2.72429918]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA52435AC0>>, X=array([2.1498433 , 2.67022555, 3.08464681, ..., 2.53605723, 2.4169789 ,\n",
      "       2.60144456]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA50A56430>>, X=array([2.14960303, 2.63169871, 3.15512372, ..., 2.63069517, 2.4914431 ,\n",
      "       2.58020228]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA52435C40>>, X=array([2.18526444, 2.71808584, 3.13689549, ..., 2.53298586, 2.32150521,\n",
      "       2.72184306]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA524359A0>>, X=array([2.51729395, 2.51729395, 2.51729395, ..., 2.51729395, 2.51729395,\n",
      "       2.51729395]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA52435CA0>>, X=array([2.51729395, 2.51729395, 2.51729395, ..., 2.51729395, 2.51729395,\n",
      "       2.51729395]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA52435AC0>>, X=array([2.51729395, 2.51729395, 2.51729395, ..., 2.51729395, 2.51729395,\n",
      "       2.51729395]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA52435C40>>, X=array([2.51729395, 2.51729395, 2.51729395, ..., 2.51729395, 2.51729395,\n",
      "       2.51729395]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA524358B0>>, X=array([2.51729395, 2.51729395, 2.51729395, ..., 2.51729395, 2.51729395,\n",
      "       2.51729395]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA52435850>>, X=array([2.51729395, 2.51729395, 2.51729395, ..., 2.51729395, 2.51729395,\n",
      "       2.51729395]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA51D58070>>, X=array([2.31379436, 3.57352208, 3.35026447, ..., 2.57391837, 2.5714563 ,\n",
      "       2.50620274]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n",
      "functools.partial(<bound method OptimizedRounder._kappa_loss of <__main__.OptimizedRounder object at 0x000001EA51D580D0>>, X=array([2.20715586, 3.57043632, 3.31861917, ..., 2.44254586, 2.40908602,\n",
      "       2.65780575]), y=array([2, 1, 4, ..., 4, 2, 1], dtype=int64))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rory\\Desktop\\work\\UNSW\\2022T2\\COMP9417\\group_project\\cs9417_aiCaramba\\regressions.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000006?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000006?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m X_train, X_test \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000006?line=9'>10</a>\u001b[0m         model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000006?line=10'>11</a>\u001b[0m         predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000006?line=12'>13</a>\u001b[0m         optR \u001b[39m=\u001b[39m OptimizedRounder()\n",
      "File \u001b[1;32mc:\\Users\\Rory\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:586\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resize_state()\n\u001b[0;32m    585\u001b[0m \u001b[39m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m n_stages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stages(\n\u001b[0;32m    587\u001b[0m     X,\n\u001b[0;32m    588\u001b[0m     y,\n\u001b[0;32m    589\u001b[0m     raw_predictions,\n\u001b[0;32m    590\u001b[0m     sample_weight,\n\u001b[0;32m    591\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[0;32m    592\u001b[0m     X_val,\n\u001b[0;32m    593\u001b[0m     y_val,\n\u001b[0;32m    594\u001b[0m     sample_weight_val,\n\u001b[0;32m    595\u001b[0m     begin_at_stage,\n\u001b[0;32m    596\u001b[0m     monitor,\n\u001b[0;32m    597\u001b[0m )\n\u001b[0;32m    599\u001b[0m \u001b[39m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39mif\u001b[39;00m n_stages \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Rory\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:663\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    656\u001b[0m     old_oob_score \u001b[39m=\u001b[39m loss_(\n\u001b[0;32m    657\u001b[0m         y[\u001b[39m~\u001b[39msample_mask],\n\u001b[0;32m    658\u001b[0m         raw_predictions[\u001b[39m~\u001b[39msample_mask],\n\u001b[0;32m    659\u001b[0m         sample_weight[\u001b[39m~\u001b[39msample_mask],\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stage(\n\u001b[0;32m    664\u001b[0m     i,\n\u001b[0;32m    665\u001b[0m     X,\n\u001b[0;32m    666\u001b[0m     y,\n\u001b[0;32m    667\u001b[0m     raw_predictions,\n\u001b[0;32m    668\u001b[0m     sample_weight,\n\u001b[0;32m    669\u001b[0m     sample_mask,\n\u001b[0;32m    670\u001b[0m     random_state,\n\u001b[0;32m    671\u001b[0m     X_csc,\n\u001b[0;32m    672\u001b[0m     X_csr,\n\u001b[0;32m    673\u001b[0m )\n\u001b[0;32m    675\u001b[0m \u001b[39m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\Rory\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:246\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    243\u001b[0m     sample_weight \u001b[39m=\u001b[39m sample_weight \u001b[39m*\u001b[39m sample_mask\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    245\u001b[0m X \u001b[39m=\u001b[39m X_csr \u001b[39mif\u001b[39;00m X_csr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\n\u001b[1;32m--> 246\u001b[0m tree\u001b[39m.\u001b[39;49mfit(X, residual, sample_weight\u001b[39m=\u001b[39;49msample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    248\u001b[0m \u001b[39m# update tree leaves\u001b[39;00m\n\u001b[0;32m    249\u001b[0m loss\u001b[39m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    250\u001b[0m     tree\u001b[39m.\u001b[39mtree_,\n\u001b[0;32m    251\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Rory\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m   1279\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m ):\n\u001b[0;32m   1281\u001b[0m     \u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \n\u001b[0;32m   1283\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1316\u001b[0m         X,\n\u001b[0;32m   1317\u001b[0m         y,\n\u001b[0;32m   1318\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1319\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1320\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Rory\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit all 3 models on all 3 datasets using the above rounding functionality\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [(vtX_train, vtX_test), (kbX_train, kbX_test), (pcaX_train, pcaX_test)]\n",
    "models = [ols, ridge, lasso, elastic, logReg]\n",
    "i = 0\n",
    "for model in models:\n",
    "    for X_train, X_test in datasets:\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(predictions, y_test)\n",
    "\n",
    "        coefs = optR.coefficients()\n",
    "        valid_p = optR.predict(predictions, coefs)\n",
    "        \n",
    "        results[i] = model.score(X_test, y_test)\n",
    "        i += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.ensemble.GradientBoostingRegressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rory\\Desktop\\work\\UNSW\\2022T2\\COMP9417\\group_project\\cs9417_aiCaramba\\regressions.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# NOTES\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=1'>2</a>\u001b[0m \u001b[39m# Problem can be treated as both linear and classifier problem. Not good in linear models. Can bucket predictions into classes.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGradientBoostingRegressor\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=4'>5</a>\u001b[0m lgb_params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mboosting_type\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mgbdt\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrandom_state\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2018\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=20'>21</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000007?line=22'>23</a>\u001b[0m lgbmodel \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mLGBMRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlgb_params)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.ensemble.GradientBoostingRegressor'"
     ]
    }
   ],
   "source": [
    "# NOTES\n",
    "# Problem can be treated as both linear and classifier problem. Not good in linear models. Can bucket predictions into classes.\n",
    "import sklearn.ensemble.GradientBoostingRegressor\n",
    "\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.005,\n",
    "    'subsample': .8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_split_gain': 0.006,\n",
    "    'min_child_samples': 150,\n",
    "    'min_child_weight': 0.1,\n",
    "    'max_depth': 17,\n",
    "    'n_estimators': 10000,\n",
    "    'num_leaves': 80,\n",
    "    'silent': -1,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 11,\n",
    "    'random_state': 2018\n",
    "    }\n",
    "\n",
    "lgbmodel = lgb.LGBMRegressor(**lgb_params)\n",
    "lgbmodel.fit(\n",
    "    vtX_train, y_train,\n",
    "    eval_set=[(vtX_test, y_test)],\n",
    "    eval_metric='rmse',\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=100\n",
    ")\n",
    "\n",
    "#model.fit(xtrain, ytrain)\n",
    "valid_preds = lgbmodel.predict(vtX_test, num_iteration=model.best_iteration_)\n",
    "print(lgbmodel.score(vtX_test, y_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rory\\Desktop\\work\\UNSW\\2022T2\\COMP9417\\group_project\\cs9417_aiCaramba\\regressions.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000005?line=0'>1</a>\u001b[0m logReg \u001b[39m=\u001b[39m GradientBoostingRegressor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000005?line=2'>3</a>\u001b[0m logReg\u001b[39m.\u001b[39mfit(X_train, Y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000005?line=4'>5</a>\u001b[0m Y_pred \u001b[39m=\u001b[39m logReg\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rory/Desktop/work/UNSW/2022T2/COMP9417/group_project/cs9417_aiCaramba/regressions.ipynb#ch0000005?line=6'>7</a>\u001b[0m \u001b[39mmax\u001b[39m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(Y_pred \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39m amax(Y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logReg = GradientBoostingRegressor()\n",
    "\n",
    "logReg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = logReg.predict(X_test)\n",
    "\n",
    "max = np.where(Y_pred == np. amax(Y_pred))\n",
    "\n",
    "wrongValues = []\n",
    "k = 0\n",
    "for i, value in enumerate(Y_pred):\n",
    "    if value > 10: \n",
    "        Y_pred[i] = 5\n",
    "        k+=1\n",
    "    if value < -5:\n",
    "        Y_pred[i] = 0\n",
    "        k+=1\n",
    "print(k)\n",
    "# for i in range(0,5):\n",
    "#     print(Y_pred[np.random.randint(0, len(Y_pred))])\n",
    "# print(len(Y_pred))\n",
    "# print(len(Y_test))\n",
    "\n",
    "results = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\n",
    "\n",
    "  \n",
    "\n",
    "print(mean_squared_error(Y_test, roundGuess(Y_pred)))\n",
    "print(r2_score(Y_test, Y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# NOTES: I tried rounding the guesses into discrete quantities, and using different regression models. Best R^2 I could get was 0.11 with gradient Boosted regression\n",
    "# Need to remove collinearity and clean data more\n",
    "# Need to try other models and maybe look at images\n",
    "# Try to get an R^2 of at least 0.3 if you can. \n",
    "# Look into this https://www.kaggle.com/competitions/petfinder-adoption-prediction/discussion/87733\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52427ff3b84206cb3d0491fd58946178d9bf6520d3aa33a25117d43f7acd7872"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
